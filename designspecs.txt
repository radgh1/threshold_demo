Plan B
To address the research scenario, we design a flexible workload distribution mechanism 
that enables dynamic control of the trade-off between human expert coverage and AI 
classifier efficiency, while ensuring reliable performance evaluation through the coverage-
accuracy curve. The core objective is to identify a principled, adaptive strategy for 
workload allocation that maximizes system reliability without overburdening human 
experts.
We begin by defining the system’s operational constraints: (1) human experts are costly 
and limited in capacity, (2) AI classifiers have high throughput but may exhibit 
unpredictable errors, and (3) the coverage-accuracy curve serves as the primary evaluation 
metric, with coverage defined as the fraction of inputs processed by humans and accuracy 
as the fraction of correct predictions among all outputs. To assess the trade-off, we must 
control the allocation rule between humans and AI across varying input distributions.
The mechanism we propose is a dynamic confidence thresholding system with adjustable 
human-in-the-loop (HITL) trigger conditions. Specifically, for each input, the AI classifier 
produces a prediction along with a confidence score. We define a tunable threshold τ such 
that if the AI’s confidence exceeds τ, the system uses the AI’s prediction; otherwise, the 
input is routed to a human expert. This threshold τ becomes the primary control variable 
for workload distribution. The system maintains a coverage ratio C(τ) = proportion of 
inputs routed to humans, and an accuracy A(τ) = overall correctness rate under threshold 
τ.
To implement this, we first collect a representative dataset D of real-world inputs, stratified 
by task complexity and domain diversity (e.g., medical diagnosis, legal document review). 
We then train a robust AI classifier on a labeled subset of D and evaluate its performance 
on a held-out test set. For each input in the test set, we record the AI’s prediction and 
confidence score. This forms the basis for simulating the coverage-accuracy curve under 
varying τ.
Next, we conduct controlled simulations across a grid of τ values (e.g., 0.1 to 0.9 in steps 
of 0.05). For each τ, we compute C(τ) and A(τ) by applying the threshold rule to the test 
set. The resulting curve reveals the trade-off: as τ increases (more confident AI decisions), 
coverage decreases but accuracy may improve or degrade depending on the AI’s 
calibration. We also compute the expected human workload as a function of τ, defined as 
the number of inputs routed to experts, to quantify efficiency gains.
To account for human variability, we introduce a stochastic human expert model. We 
simulate human performance using a probabilistic response function: for each input, the 
expert has a base accuracy p_h and a workload cost c_h (e.g., time per decision). We 
further model expert fatigue by allowing accuracy to degrade over time when processing 
consecutive inputs. This allows us to evaluate system performance under realistic 
operational conditions, not just static accuracy.
We then extend the framework to adaptive thresholding. Instead of a fixed τ, we allow τ to 
vary based on real-time feedback: for example, if a human expert’s error rate exceeds a threshold, the system lowers τ to increase human involvement. We implement a feedback 
loop that monitors human performance and adjusts τ accordingly. This adaptive 
mechanism enables the system to self-regulate based on changing task demands or 
expert availability.
To validate the mechanism, we conduct a controlled user study with expert participants 
(e.g., domain experts in healthcare or law) who label a subset of D. We compare the 
performance of the fixed-threshold and adaptive systems under identical input streams. 
We measure coverage, accuracy, and human workload across conditions. We also assess 
system responsiveness and stability during adaptation.
Finally, we analyze the results through statistical modeling. We fit a piecewise regression 
model to the coverage-accuracy curve to identify optimal τ values for different operational 
goals (e.g., maximum accuracy under minimum human coverage). We also examine the 
sensitivity of performance to changes in AI confidence calibration and human error rates.
This plan enables systematic exploration of the human-AI workload trade-off. By using a 
tunable threshold as the control variable and grounding it in empirical data and human-in-
the-loop feedback, the mechanism supports flexible, real-time adaptation. The method is 
transparent, scalable, and directly interpretable via the coverage-accuracy curve, making it 
suitable for deployment in real-world L2D systems.